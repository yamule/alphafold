{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Functions for building the input features for the AlphaFold model.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Mapping, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Internal import (7716).\n",
    "\n",
    "from alphafold.common import residue_constants\n",
    "from alphafold.data import parsers\n",
    "from alphafold.data import templates\n",
    "from alphafold.data.tools import hhblits\n",
    "from alphafold.data.tools import hhsearch\n",
    "from alphafold.data.tools import jackhmmer\n",
    "\n",
    "FeatureDict = Mapping[str, np.ndarray]\n",
    "\n",
    "\n",
    "def make_sequence_features(\n",
    "        sequence: str, description: str, num_res: int) -> FeatureDict:\n",
    "    \"\"\"Constructs a feature dict of sequence features.\"\"\"\n",
    "    features = {}\n",
    "    features['aatype'] = residue_constants.sequence_to_onehot(\n",
    "            sequence=sequence,\n",
    "            mapping=residue_constants.restype_order_with_x,\n",
    "            map_unknown_to_x=True)\n",
    "    features['between_segment_residues'] = np.zeros((num_res,), dtype=np.int32)\n",
    "    features['domain_name'] = np.array([description.encode('utf-8')],\n",
    "                                                                         dtype=np.object_)\n",
    "    features['residue_index'] = np.array(range(num_res), dtype=np.int32)\n",
    "    features['seq_length'] = np.array([num_res] * num_res, dtype=np.int32)\n",
    "    features['sequence'] = np.array([sequence.encode('utf-8')], dtype=np.object_)\n",
    "    return features\n",
    "\n",
    "\n",
    "def make_msa_features(\n",
    "        msas: Sequence[Sequence[str]],\n",
    "        deletion_matrices: Sequence[parsers.DeletionMatrix]) -> FeatureDict:\n",
    "    \"\"\"Constructs a feature dict of MSA features.\"\"\"\n",
    "    if not msas:\n",
    "        raise ValueError('At least one MSA must be provided.')\n",
    "\n",
    "    int_msa = []\n",
    "    deletion_matrix = []\n",
    "    seen_sequences = set()\n",
    "    for msa_index, msa in enumerate(msas):\n",
    "        if not msa:\n",
    "            raise ValueError(f'MSA {msa_index} must contain at least one sequence.')\n",
    "        for sequence_index, sequence in enumerate(msa):\n",
    "            if sequence in seen_sequences:\n",
    "                continue\n",
    "            seen_sequences.add(sequence)\n",
    "            int_msa.append(\n",
    "                    [residue_constants.HHBLITS_AA_TO_ID[res] for res in sequence])\n",
    "            deletion_matrix.append(deletion_matrices[msa_index][sequence_index])\n",
    "\n",
    "    num_res = len(msas[0][0])\n",
    "    num_alignments = len(int_msa)\n",
    "    features = {}\n",
    "    features['deletion_matrix_int'] = np.array(deletion_matrix, dtype=np.int32)\n",
    "    features['msa'] = np.array(int_msa, dtype=np.int32)\n",
    "    features['num_alignments'] = np.array(\n",
    "            [num_alignments] * num_res, dtype=np.int32)\n",
    "    return features\n",
    "\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"Runs the alignment tools and assembles the input features.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 jackhmmer_binary_path: str,\n",
    "                 hhblits_binary_path: str,\n",
    "                 hhsearch_binary_path: str,\n",
    "                 uniref90_database_path: str,\n",
    "                 mgnify_database_path: str,\n",
    "                 bfd_database_path: Optional[str],\n",
    "                 uniclust30_database_path: Optional[str],\n",
    "                 small_bfd_database_path: Optional[str],\n",
    "                 pdb70_database_path: str,\n",
    "                 template_featurizer: templates.TemplateHitFeaturizer,\n",
    "                 use_small_bfd: bool,\n",
    "                 mgnify_max_hits: int = 501,\n",
    "                 uniref_max_hits: int = 10000,\n",
    "                 use_a3m:bool = False,\n",
    "                 search_templates:bool = True\n",
    "                ):\n",
    "        \"\"\"Constructs a feature dict for a given FASTA file.\"\"\"\n",
    "        self.use_a3m = use_a3m;\n",
    "        self.search_templates = search_template;\n",
    "        if not use_a3m:\n",
    "            self._use_small_bfd = use_small_bfd\n",
    "            self.jackhmmer_uniref90_runner = jackhmmer.Jackhmmer(\n",
    "                    binary_path=jackhmmer_binary_path,\n",
    "                    database_path=uniref90_database_path)\n",
    "            if use_small_bfd:\n",
    "                self.jackhmmer_small_bfd_runner = jackhmmer.Jackhmmer(\n",
    "                        binary_path=jackhmmer_binary_path,\n",
    "                        database_path=small_bfd_database_path)\n",
    "            else:\n",
    "                self.hhblits_bfd_uniclust_runner = hhblits.HHBlits(\n",
    "                        binary_path=hhblits_binary_path,\n",
    "                        databases=[bfd_database_path, uniclust30_database_path])\n",
    "            self.jackhmmer_mgnify_runner = jackhmmer.Jackhmmer(\n",
    "                    binary_path=jackhmmer_binary_path,\n",
    "                    database_path=mgnify_database_path);\n",
    "            self.mgnify_max_hits = mgnify_max_hits\n",
    "            self.uniref_max_hits = uniref_max_hits\n",
    "            \n",
    "        if search_templates:\n",
    "            try:\n",
    "                self.jackhmmer_uniref90_runner;\n",
    "            except NameError:\n",
    "                self.jackhmmer_uniref90_runner = jackhmmer.Jackhmmer(\n",
    "                    binary_path=jackhmmer_binary_path,\n",
    "                    database_path=uniref90_database_path);\n",
    "            self.hhsearch_pdb70_runner = hhsearch.HHSearch(\n",
    "                    binary_path=hhsearch_binary_path,\n",
    "                    databases=[pdb70_database_path])\n",
    "            self.template_featurizer = template_featurizer\n",
    "\n",
    "    def process(self, input_fasta_path: str, msa_output_dir: str) -> FeatureDict:\n",
    "        \"\"\"Runs alignment tools on the input sequence and creates features.\"\"\"\n",
    "        if not self.use_a3m:\n",
    "            with open(input_fasta_path) as f:\n",
    "                input_fasta_str = f.read()\n",
    "            input_seqs, input_descs = parsers.parse_fasta(input_fasta_str)\n",
    "            if len(input_seqs) != 1:\n",
    "                raise ValueError(\n",
    "                        f'More than one input sequence found in {input_fasta_path}.')\n",
    "            input_sequence = input_seqs[0]\n",
    "            input_description = input_descs[0]\n",
    "            num_res = len(input_sequence)\n",
    "\n",
    "            jackhmmer_uniref90_result = self.jackhmmer_uniref90_runner.query(\n",
    "                    input_fasta_path)[0]\n",
    "            jackhmmer_mgnify_result = self.jackhmmer_mgnify_runner.query(\n",
    "                    input_fasta_path)[0]\n",
    "\n",
    "            if self.search_templates:\n",
    "                uniref90_msa_as_a3m = parsers.convert_stockholm_to_a3m(\n",
    "                        jackhmmer_uniref90_result['sto'], max_sequences=self.uniref_max_hits)\n",
    "                hhsearch_result = self.hhsearch_pdb70_runner.query(uniref90_msa_as_a3m)\n",
    "\n",
    "            uniref90_out_path = os.path.join(msa_output_dir, 'uniref90_hits.sto')\n",
    "            with open(uniref90_out_path, 'w') as f:\n",
    "                f.write(jackhmmer_uniref90_result['sto'])\n",
    "\n",
    "            mgnify_out_path = os.path.join(msa_output_dir, 'mgnify_hits.sto')\n",
    "            with open(mgnify_out_path, 'w') as f:\n",
    "                f.write(jackhmmer_mgnify_result['sto'])\n",
    "\n",
    "            uniref90_msa, uniref90_deletion_matrix, _ = parsers.parse_stockholm(\n",
    "                    jackhmmer_uniref90_result['sto'])\n",
    "            mgnify_msa, mgnify_deletion_matrix, _ = parsers.parse_stockholm(\n",
    "                    jackhmmer_mgnify_result['sto'])\n",
    "            hhsearch_hits = parsers.parse_hhr(hhsearch_result)\n",
    "            mgnify_msa = mgnify_msa[:self.mgnify_max_hits]\n",
    "            mgnify_deletion_matrix = mgnify_deletion_matrix[:self.mgnify_max_hits]\n",
    "\n",
    "            if self._use_small_bfd:\n",
    "                jackhmmer_small_bfd_result = self.jackhmmer_small_bfd_runner.query(\n",
    "                        input_fasta_path)[0]\n",
    "\n",
    "                bfd_out_path = os.path.join(msa_output_dir, 'small_bfd_hits.a3m')\n",
    "                with open(bfd_out_path, 'w') as f:\n",
    "                    f.write(jackhmmer_small_bfd_result['sto'])\n",
    "\n",
    "                bfd_msa, bfd_deletion_matrix, _ = parsers.parse_stockholm(\n",
    "                        jackhmmer_small_bfd_result['sto'])\n",
    "            else:\n",
    "                hhblits_bfd_uniclust_result = self.hhblits_bfd_uniclust_runner.query(\n",
    "                        input_fasta_path)\n",
    "\n",
    "                bfd_out_path = os.path.join(msa_output_dir, 'bfd_uniclust_hits.a3m')\n",
    "                with open(bfd_out_path, 'w') as f:\n",
    "                    f.write(hhblits_bfd_uniclust_result['a3m'])\n",
    "\n",
    "                bfd_msa, bfd_deletion_matrix = parsers.parse_a3m(\n",
    "                        hhblits_bfd_uniclust_result['a3m']);\n",
    "                all_msas = (uniref90_msa, bfd_msa, mgnify_msa);\n",
    "                all_delmat = (uniref90_deletion_matrix,\n",
    "                             bfd_deletion_matrix,\n",
    "                             mgnify_deletion_matrix);\n",
    "        else:\n",
    "            \n",
    "            with open(input_fasta_path) as f:\n",
    "                input_fasta_str = f.read()\n",
    "            input_seqs, input_descs = parsers.parse_fasta(input_fasta_str)\n",
    "            \n",
    "            input_sequence = input_seqs[0]\n",
    "            input_description = input_descs[0]\n",
    "            num_res = len(input_sequence)\n",
    "            del input_seqs;\n",
    "            del input_description;\n",
    "            \n",
    "            a3mm, delmat = parsers.parse_a3m(input_fasta_str);\n",
    "            \n",
    "            all_msas = (a3mm,);\n",
    "            all_delmat = (delmat,)\n",
    "            if self.search_templates:            \n",
    "                jackhmmer_uniref90_result = self.jackhmmer_uniref90_runner.query(\n",
    "                        input_fasta_path)[0]\n",
    "                jackhmmer_mgnify_result = self.jackhmmer_mgnify_runner.query(\n",
    "                        input_fasta_path)[0]\n",
    "\n",
    "                uniref90_msa_as_a3m = parsers.convert_stockholm_to_a3m(\n",
    "                        jackhmmer_uniref90_result['sto'], max_sequences=self.uniref_max_hits)\n",
    "                hhsearch_result = self.hhsearch_pdb70_runner.query(uniref90_msa_as_a3m)\n",
    "        if self.use_templates:     \n",
    "            templates_result = self.template_featurizer.get_templates(\n",
    "                    query_sequence=input_sequence,\n",
    "                    query_pdb_code=None,\n",
    "                    query_release_date=None,\n",
    "                    hits=hhsearch_hits);\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            for name in list(templates.TEMPLATE_FEATURES):\n",
    "                template_features[name] = np.array([], dtype=templates.TEMPLATE_FEATURES[name]);\n",
    "            templates_result = TemplateSearchResult(\n",
    "                features=template_features, errors=[], warnings=[]);\n",
    "\n",
    "        sequence_features = make_sequence_features(\n",
    "                sequence=input_sequence,\n",
    "                description=input_description,\n",
    "                num_res=num_res)\n",
    "\n",
    "        msa_features = make_msa_features(\n",
    "                msas=all_msas,\n",
    "                deletion_matrices=all_delmat)\n",
    "\n",
    "        return {**sequence_features, **msa_features, **templates_result.features}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
